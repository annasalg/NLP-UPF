# -*- coding: utf-8 -*-
"""EX1_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jOqb-YKAMgCCsBTWl_6a5MDxhVn37-N3

# Downloading libraries, packages and files
"""

!python -m spacy download es_core_news_sm --quiet
import en_core_web_sm
import spacy
from collections import Counter
import re
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
!pip install adjustText --quiet
from adjustText import adjust_text

import gdown

#the code below will download the datasets to the colab enviroment
!gdown 18tG4yI3WiVVk5-TUjiMq_Pv_5fEb_oUi #mecano_lyrics.txt
!gdown 1NAJQMNLszg8FKr9NH7Z-5NBjOUcqiE-W #tswift_lyrics.txt

print('Done!')

"""
# SPANISH / MECANO'S LYRICS
"""

#we load spaCy's model for Spanish
nlp = spacy.load("es_core_news_sm")

#we load the txt file with the lyrics and read it
mecano_lyrics = "mecano_lyrics.txt"
with open(mecano_lyrics, 'r', encoding='utf-8') as file:
    data = file.read()

#we use a regular expression to remove the Unicode characters and make all text lower case
pattern = re.compile(r'[\u2028\n]+')
cleaned_text_m = re.sub(pattern, '', data)
cleaned_text_m = cleaned_text_m.lower()

#save the cleaned text as a spaCy doc object
doc_m = nlp(cleaned_text_m)

#tokenize all words, excluding puntuation
m_all_words = [token.text
         for token in doc_m
         if not token.is_punct]

#tokenize all words that aren't stop words or puntuation
m_words = [token.text
         for token in doc_m
         if not token.is_stop and
         not token.is_punct]

#get only tokens that are nouns; not necessary to analyze Zipf's Law, just for curiosity
m_nouns = [token.text
         for token in doc_m
         if (not token.is_stop and
             not token.is_punct and
             token.pos_ == "NOUN")]

#we print the most common words (including stop words)
m_all_word_freq = Counter(m_all_words)
m_common_words_all = m_all_word_freq.most_common(10)
print(m_common_words_all)

#we print the most common words (excluding stop words)
m_word_freq = Counter(m_words)
m_common_words = m_word_freq.most_common(10)
print(m_common_words)

#we print the most common nouns
m_noun_freq = Counter(m_nouns)
m_common_nouns = m_noun_freq.most_common(10)
print(m_common_nouns)

sns.set_theme(style="darkgrid")
sns.set_palette("husl", 1)

#we create a dataframe with the words variable and the counter for frequency
m_df_stop = pd.DataFrame.from_records(list(dict(Counter(m_all_words)).items()), columns=['word','frequency'])

#sort it in descending order based on frequency
m_df_stop = m_df_stop.sort_values(by=['frequency'], ascending=False)

#create a column 'rank', words are ranked based on frequency
m_df_stop['rank'] = list(range(1, len(m_df_stop) + 1))

#we create a scatter plot
sns.relplot(x="rank", y="frequency", data=m_df_stop);

#we add labels to each data point and adjust their placement
texts = []
for i, row in m_df_stop.iterrows():
  if row['frequency'] > 100:
    texts.append(plt.text(row['rank'], row['frequency'], row['word'], ha='left', va='center', size=10))

adjust_text(texts, arrowprops=dict(arrowstyle="-", color='black', lw=0.5))

plt.title("Mecano")
plt.show()

#we create a dataframe for the non-stop words
m_df_ns = pd.DataFrame.from_records(list(dict(Counter(m_words)).items()), columns=['word','frequency'])
m_df_ns = m_df_ns.sort_values(by=['frequency'], ascending=False)
m_df_ns['rank'] = list(range(1, len(m_df_ns) + 1))


#we check the descriptive of the more and less common words on the dataset
print("first 10 words")
print(m_df_stop[:10])
print()
print("last 10 words")
print(m_df_stop[-10:])

#we check the more common non-stop words
print(m_df_ns[:5])

"""
# ENGLISH / TAYLOR SWIFT'S LYRICS
"""

#we load SpaCy's model for English
nlp = en_core_web_sm.load()

#we load the txt file with the lyrics and read it
tswift_lyrics = "tswift_lyrics.txt"
with open(tswift_lyrics, 'r', encoding='utf-8') as file:
    data = file.read()

#we use a regular expression to remove the Unicode characters and make all text lower case
pattern = re.compile(r'[\u2028\n]+|\[.*?\]')
cleaned_text_t = re.sub(pattern, ' ', data)
cleaned_text_t = cleaned_text_t.lower()

#save the cleaned text as a spaCy doc object
doc_t = nlp(cleaned_text_t)

#tokenize all words, excluding puntuation
t_all_words = [token.text
         for token in doc_t
         if not token.is_punct and
         not token.is_space]

#tokenize all words that aren't stop words or puntuation
t_words = [token.text
         for token in doc_t
         if not token.is_stop and
         not token.is_punct]

#get only tokens that are nouns; not necessary to analyze Zipf's Law, just for curiosity
t_nouns = [token.text
         for token in doc_t
         if (not token.is_stop and
             not token.is_punct and
             token.pos_ == "NOUN")]

#we print the most common words (including stop words)
t_all_word_freq = Counter(t_all_words)
t_common_words_all = t_all_word_freq.most_common(10)
print(t_common_words_all)

#we print the most common words (excluding stop words)
t_word_freq = Counter(t_words)
t_common_words = t_word_freq.most_common(5)
print(t_common_words)

#we print the most common nouns
t_noun_freq = Counter(t_nouns)
t_common_nouns = t_noun_freq.most_common(5)
print(t_common_nouns)

sns.set_theme(style="darkgrid")

#we create a dataframe with the words variable and the counter for frequency
t_df_stop = pd.DataFrame.from_records(list(dict(Counter(t_all_words)).items()), columns=['word','frequency'])
#sort it in descending order based on frequency
t_df_stop = t_df_stop.sort_values(by=['frequency'], ascending=False)
#create a column 'rank', words are ranked based on frequency
t_df_stop['rank'] = list(range(1, len(t_df_stop) + 1))
#we create a scatter plot
sns.relplot(x="rank", y="frequency", data=t_df_stop);
#we add labels to each data point and adjust their placement
texts = []
for i, row in t_df_stop.iterrows():
  if row['frequency'] > 800:
    texts.append(plt.text(row['rank'], row['frequency'], row['word'], ha='left', va='center', size=10))

adjust_text(texts, arrowprops=dict(arrowstyle="-", color='black', lw=0.5))

plt.title("Taylor Swift")
plt.show()

#we create a dataframe for the non-stop words
t_df_ns = pd.DataFrame.from_records(list(dict(Counter(t_words)).items()), columns=['word','frequency'])
t_df_ns = t_df_ns.sort_values(by=['frequency'], ascending=False)
t_df_ns['rank'] = list(range(1, len(t_df_ns) + 1))
t_df_ns = t_df_ns.drop(335)
#sns.relplot(x="rank", y="frequency", data=t_df_ns);

#plt.show()
#plt.close()

#we check the descriptive of the more and less common words on the dataset
print("first 10 words")
print(t_df_stop[:10])
print()
print("last 10 words")
print(t_df_stop[-10:])

#we check the more common non-stop words
print(t_df_ns[:5])